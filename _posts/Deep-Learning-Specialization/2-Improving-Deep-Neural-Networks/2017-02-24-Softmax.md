---
layout: post
title: Softmax
tags: Deep-Learning
mathjax: true
categories: Deep-Learning
excerpt: <p></p>
published: true
---
### Softmax Regression vs Logistic (Binary) Regression
There are two types of classification: binary (what all examples have been up to now) and softmax. Softmax regression is a generalization of logistic regression that is used for multiclass classification.

So, instead of analyzing whether an image depicts a cat or does not, softmax regression can assign the cat class a number, and can assign other animals numbers as well. The result is a change in the output of the neural network to be a vector of which class is predicted, rather than if the class is present or not.

Let `C = no. of classes`  
Then, the length of the output vector, Ny = `C`

Softmax activation equations:
```
t = e^(Z[L])                      # shape(C, m)
A[L] = e^(Z[L]) / sum(t)          # shape(C, m), sum(t) - sum of t's for each example (shape (1, m))
```

### Training the Softmax Classifier
Softmax is a generalization of logistic activation functions from `C=2` to `C=n` classes.  

The loss function used with softmax:
```
L(y, y_hat) = - sum(y[j] * log(y_hat[j])) # j = 0 to C-1
```
The cost function used with softmax:
```
J(w[1], b[1], ...) = - 1 / m * (sum(L(y[i], y_hat[i]))) # i = 0 to m
```
Backprop with softmax:
```
dZ[L] = Y_hat - Y
```
Derivative of softmax function:
```
Y_hat * (1 - Y_hat)
```