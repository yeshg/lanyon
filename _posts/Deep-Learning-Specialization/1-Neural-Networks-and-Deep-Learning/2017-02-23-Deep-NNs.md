---
layout: post
title: Deep Neural Networks
tags: Deep-Learning
mathjax: true
excerpt:
    <p>The true success of machine learning has been in deep neural networks. They have several applications in computer vision, natural language processing, and generative systems. A Deep neural network is a neural network with three or more layers.</p>
---

The true success of machine learning has been in deep neural networks. They have several applications in computer vision, natural language processing, and generative systems. A Deep neural network is a neural network with three or more layers.

### Notation
- We'll use `L` to represent the number of layers.
- `n[l]` is the number of neurons in a specific layer `l`.
- `n[0]` denotes the number of neurons input layer. `n[L]` denotes the number of neurons in output layer.
- `g[l]` is the activation function.
- `a[l] = g[l](z[l])`
- `w[l]` weights is used for `z[l]`
- `x = a[0]`, `a[l] = y'`

In our construction of the deep neural network we have four major parts:
- A vector `n` of shape `(1, L+1)` -- this goes from the input or 0th layer to the Lth layer
- A vector `g` of shape `(1, L)`
- A list of different shapes `w` based on the number of neurons on the previous and the current layer.
- A list of different shapes `b` based on the number of neurons on the current layer.

### Forward Propagation in a Deep NN
#### General rule for one input:
```
z[l] = W[l]a[l-1] + b[l]
a[l] = g[l](a[l])
```
#### General rule for `m` inputs:
```
Z[l] = W[l]A[l-1] + B[l]
A[l] = g[l](A[l])
```

Note that despite our efforts to vectorize everything, we can't compute forward propagation for all of the layers without a for loop.

### Why Deep Representations?
- Deep NN makes relations with data from simpler to complex. In each layer it tries to make a relations between the previous layer.
- Face recognition application:
- Image --> Edges --> Face parts --> Faces --> desired face
- Audio recognition application:
- Audio --> Low level sound features like (sss,bb) --> Phonemes --> Words --> Sentences
- Neural Researchers thinks that deep neural networks thinks like brains (Simple ==> Complex)
- Circuit theory and deep learning:
![](Images/circuitTheoryAndDeepNNs.png)

### Building Blocks of Deep Neural Networks
![](Images/buildingBlocks.png)

### Forward and Backward Propagation Pseudo Code
In real applications, most people use frameworks rather than Python + NumPy, but knowing what those frameworks do is still important.

#### Forward Propagation for Layer `l`
```
Input  A[l-1]
Z[l] = W[l]A[l-1] + b[l]
A[l] = g[l](Z[l])
Output A[l], cache(Z[l])
```

#### Backward Propagation for Layer `l`
```
Input da[l], Caches
dZ[l] = dA[l] * g'[l](Z[l])
dW[l] = (dZ[l]A[l-1].T) / m
db[l] = sum(dZ[l])/m                # Dont forget axis=1, keepdims=True
dA[l-1] = w[l].T * dZ[1]            # The multiplication here are a dot product.
Output dA[l-1], dW[1], db[1]
```

#### Loss Calculation
```
dA[L] = sum(-(y/a) + ((1-y)/(1-a)))
```

### Parameters vs Hyperparameters
**Parameters** are learned during training.
**Hyperparameters** are set by the user, they control the algorithms.
- Learning rate ($\alpha$)
- Number of iterations
- Number of hidden layers `L`
- Number of hidden units `n`
- Choice of activation functions (which can vary from layer to layer based on the NN architecture)